{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDPFL on the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to run LDPFL on the CIFAR-10 Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "from CIFAR10_FL import FedMLFunc as fl\n",
    "from CIFAR10_FL import Net as Net\n",
    "fl = fl()\n",
    "\n",
    "from LDP_Functions import SRR\n",
    "from LDP_Functions import LDP_FL\n",
    "from LDP_Functions import GRR\n",
    "\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Getting GPU usage information before computation\n",
    "if device.type == 'cuda':\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        print('Memory Usage of device :', i)\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "num_classes = 10 \n",
    "\n",
    "# Client training settings\n",
    "epochs = 50 # The number of epochs for the clients during FL  \n",
    "BATCH_SIZE = 64\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# FL settings\n",
    "num_of_clients = 100\n",
    "selection_ratio = 0.5\n",
    "num_rounds = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_datasets():\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "      [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = CIFAR10(\"./dataset\", train=True, download=True, transform=train_transform)\n",
    "    test_dataset = CIFAR10(\"./dataset\", train=False, download=True, transform=test_transform)\n",
    "    \n",
    "    n_samples = len(train_dataset) // num_of_clients\n",
    "    class_counts = torch.zeros(10)\n",
    "    for i in range(len(train_dataset)):\n",
    "        class_counts[train_dataset[i][1]] += 1\n",
    "\n",
    "    # Divide the samples for each class into n parts\n",
    "    class_indices = {}\n",
    "    for i in range(len(train_dataset)):\n",
    "        label = train_dataset[i][1]\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(i)\n",
    "\n",
    "    for label in class_indices:\n",
    "        np.random.shuffle(class_indices[label])\n",
    "        class_indices[label] = [class_indices[label][i::num_of_clients] for i in range(num_of_clients)]\n",
    "\n",
    "    # Create datasets for each client by combining the parts from each class\n",
    "    datasets_list = []\n",
    "    for i in range(num_of_clients):\n",
    "        indices = []\n",
    "        for label in class_indices:\n",
    "            indices += class_indices[label][i]\n",
    "        dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        datasets_list.append(dataloader)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    return datasets_list, test_loader\n",
    "\n",
    "\n",
    "trainloaders, testloader = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring the global model and preparing data for federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of optimizers and models, to free up memory\n",
    "try:\n",
    "    for o in opt:\n",
    "        del o\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    for model in client_models:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del global_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        print('Memory Usage of device :', i)\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Instantiate models and optimizers\n",
    "global_model = nn.DataParallel(Net()).cuda()\n",
    "client_models = [nn.DataParallel(Net()) for _ in range(num_of_clients)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "opt = [optim.Adam(model.parameters(), lr=0.0002) for model in client_models]\n",
    "# opt = [optim.SGD(model.parameters(), lr=0.004, momentum=0.9, weight_decay=5e-4) for model in client_models]\n",
    "\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        print('Memory Usage of device :', i)\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldp_func = SRR(c = 0.0, r = 0.075, epsilon = 5, delta_d = 30 , precision = 4, m = 10)\n",
    "\n",
    "# ldp_func = GRR(c = 0.0, r = 0.075, epsilon = 3, precision = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "f1 = plt.figure()\n",
    "temp = []\n",
    "for i in range(10):\n",
    "    for j in range(ldp_func.group_sizes[i]):\n",
    "        temp.append(ldp_func.probs[i])\n",
    "dom = ldp_func.data[0.075]\n",
    "pr = []\n",
    "for i in range(len(dom)):\n",
    "    pr.append([dom[i], temp[i]])\n",
    "pr = sorted(pr, key= lambda x: x[0])\n",
    "plt.plot([x[0] for x in pr], [x[1] for x in pr], label='Probability Distribution', linestyle = 'solid')\n",
    "# plt.plot(np.arange(1, num_rounds+1), loss_test_collect, label='Test', linestyle = 'solid')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Domain')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('probdist3.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Federated Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDP_FUNCS = {\n",
    "    \"[0.0, 0.3]\" : SRR(c = 0.0, r = 0.3, epsilon = 10, delta_d = 0 , precision = 4, m = 10),\n",
    "    \"[0.0, 0.2]\" : SRR(c = 0.0, r = 0.2, epsilon = 10, delta_d = 0 , precision = 4, m = 10),\n",
    "    \"[0.0, 0.075]\" : SRR(c = 0.0, r = 0.075, epsilon = 10, delta_d = 0 , precision = 4, m = 10),\n",
    "    \"[0.0, 0.075, 5]\" : SRR(c = 0.0, r = 0.075, epsilon = 10, delta_d = 0 , precision = 5, m = 10),\n",
    "    \"[0.0, 0.05]\" : SRR(c = 0.0, r = 0.05, epsilon = 10, delta_d = 0 , precision = 4, m = 10),\n",
    "    \"[0.0, 0.05, 5]\" : SRR(c = 0.0, r = 0.05, epsilon = 10, delta_d = 0 , precision = 5, m = 10),\n",
    "    \"[0.0, 0.03, 5]\" : SRR(c = 0.0, r = 0.03, epsilon = 10, delta_d = 0 , precision = 5, m = 10),\n",
    "#     \"[0.0, 0.04]\" : SRR(c = 0.0, r = 0.04, epsilon = 4, delta_d = 0 , precision = 4, m = 10),\n",
    "#     \"[0.0, 0.04, 5]\" : SRR(c = 0.0, r = 0.04, epsilon = 4, delta_d = 0 , precision = 5, m = 10),\n",
    "    \"[1.0, 0.1]\" : SRR(c = 1.0, r = 0.1, epsilon = 10, delta_d = 0 , precision = 4, m = 10),\n",
    "    \"[1.15, 0.05]\" : SRR(c = 1.15, r = 0.05, epsilon = 10, delta_d = 0 , precision = 4, m = 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Ranges\n",
    "\n",
    "ranges = {\n",
    "    \"module.conv_layers.0.weight\" : LDP_FUNCS[\"[0.0, 0.3]\"],\n",
    "    \"module.conv_layers.0.bias\" : LDP_FUNCS[\"[0.0, 0.2]\"],\n",
    "    \"module.conv_layers.1.weight\" : LDP_FUNCS[\"[1.0, 0.1]\"],\n",
    "    \"module.conv_layers.1.bias\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.3.weight\" : LDP_FUNCS[\"[0.0, 0.075, 5]\"],\n",
    "    \"module.conv_layers.3.bias\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.4.weight\" : LDP_FUNCS[\"[1.0, 0.1]\"],\n",
    "    \"module.conv_layers.4.bias\" : LDP_FUNCS[\"[0.0, 0.05]\"],\n",
    "    \"module.conv_layers.7.weight\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.7.bias\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.8.weight\" : LDP_FUNCS[\"[1.0, 0.1]\"],\n",
    "    \"module.conv_layers.8.bias\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.10.weight\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.conv_layers.10.bias\" : LDP_FUNCS[\"[0.0, 0.05, 5]\"],\n",
    "    \"module.conv_layers.11.weight\" : LDP_FUNCS[\"[1.0, 0.1]\"],\n",
    "    \"module.conv_layers.11.bias\" : LDP_FUNCS[\"[0.0, 0.2]\"],\n",
    "    \"module.fc_layers.0.weight\" : LDP_FUNCS[\"[0.0, 0.03, 5]\"],\n",
    "    \"module.fc_layers.0.bias\" : LDP_FUNCS[\"[0.0, 0.03, 5]\"],\n",
    "    \"module.fc_layers.1.weight\" : LDP_FUNCS[\"[1.15, 0.05]\"],\n",
    "    \"module.fc_layers.1.bias\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "    \"module.fc_layers.3.weight\" : LDP_FUNCS[\"[0.0, 0.2]\"],\n",
    "    \"module.fc_layers.3.bias\" : LDP_FUNCS[\"[0.0, 0.05]\"],\n",
    "#     \"module.features5.2.weight\" : LDP_FUNCS[\"[0.0, 0.05, 5]\"],\n",
    "#     \"module.features5.2.bias\" : LDP_FUNCS[\"[0.0, 0.05, 5]\"],\n",
    "#     \"module.features5.4.weight\" : LDP_FUNCS[\"[0.0, 0.05, 5]\"],\n",
    "#     \"module.features5.4.bias\" : LDP_FUNCS[\"[0.0, 0.05, 5]\"],\n",
    "#     \"module.classifier.0.weight\" : LDP_FUNCS[\"[0.0, 0.075, 5]\"],\n",
    "#     \"module.classifier.0.bias\" : LDP_FUNCS[\"[0.0, 0.075, 5]\"],\n",
    "#     \"module.classifier.3.weight\" : LDP_FUNCS[\"[0.0, 0.05]\"],\n",
    "#     \"module.classifier.3.bias\" : LDP_FUNCS[\"[0.0, 0.05]\"],\n",
    "#     \"module.classifier.6.weight\" : LDP_FUNCS[\"[0.0, 0.075]\"],\n",
    "#     \"module.classifier.6.bias\" : LDP_FUNCS[\"[0.0, 0.05]\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acc_train_collect = []\n",
    "acc_test_collect = []\n",
    "loss_train_collect = []\n",
    "loss_test_collect = []\n",
    "\n",
    "with open('cifar-10-experiment.txt', 'a+') as f:\n",
    "    f.write('Experiment with 100 clients, 0.5 selection rate and noise-free\\n\\n')\n",
    "\n",
    "index = 0\n",
    "\n",
    "for r in tqdm(range(num_rounds)):\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "    # select clients randomly\n",
    "    client_idx = np.random.permutation(num_of_clients)[:int(selection_ratio*num_of_clients)]\n",
    "    \n",
    "    trainloss = 0\n",
    "    trainacc = 0\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    qwerty = 0\n",
    "    \n",
    "    for i in client_idx:  \n",
    "        count += 1\n",
    "        client_models[i].cuda()\n",
    "        opt_state = opt[i].state_dict()\n",
    "        # new_opt = optim.SGD(client_models[i].parameters(), lr=0.004, momentum=0.9, weight_decay=5e-4)\n",
    "        new_opt = optim.Adam(client_models[i].parameters(), lr=0.0002)\n",
    "        new_opt.load_state_dict(opt_state)\n",
    "        [loss,acc]= fl.client_update(client_models[i], new_opt, trainloaders[i], epochs, device)    \n",
    "        client_models[i].to('cpu')\n",
    "        opt[i].load_state_dict(new_opt.state_dict())\n",
    "        # print(f\"Training Loss: {loss} \\t Training Accuracy: {acc}\")\n",
    "        \n",
    "        trainloss += loss\n",
    "        trainacc += acc\n",
    "        qwerty += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"{qwerty}. Client {i} Train accuracy : {acc}\\n\")\n",
    "        \n",
    "        \n",
    "    # server aggregate\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #  fl.server_aggregate(global_model, [client_models[i] for i in client_idx], client_models)\n",
    "    fl.server_aggregate_srr_adaptive(global_model, [client_models[i] for i in client_idx], client_models, ranges)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Time taken for server aggregation : {(end-start)/60} minutes.\")\n",
    "    \n",
    "    test_loss, test_acc = fl.test(global_model, testloader, device)\n",
    "    \n",
    "    print(f\"Round {index}\")\n",
    "    print('Avg Train loss %0.3g - Avg Train accuracy: %0.3g  - Test loss %0.3g - Test accuracy: %0.3f' % (trainloss / len(client_idx), trainacc / len(client_idx), test_loss, test_acc))\n",
    "    \n",
    "    with open('cifar-10-experiment.txt', 'a+') as f:\n",
    "        f.write(f\"Round{index}: Test Loss : {test_loss} | Test Accuracy: {test_acc}\\n\")\n",
    "    \n",
    "    acc_train_collect.append(trainacc / len(client_idx))\n",
    "    acc_test_collect.append(test_acc)\n",
    "    loss_train_collect.append(trainloss / len(client_idx))\n",
    "    loss_test_collect.append(test_loss)\n",
    "    \n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    \n",
    "if device.type == 'cuda':\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        print('Memory Usage of device :', i)\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "print(\"Training and Evaluation completed!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in global_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, \"\\n---\\n\", param.data)\n",
    "        print('\\n\\nNEXT LAYER\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY TESTING SCRIPT BELOW\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# # Instantiate models and optimizers\n",
    "# global_model = nn.DataParallel(Net()).cuda()\n",
    "# client_models = [nn.DataParallel(Net()) for _ in range(50)]\n",
    "\n",
    "# round_no = 14\n",
    "# for i, model in enumerate(client_models):\n",
    "#     client_models[i].load_state_dict(torch.load(f\"models/round_{round_no+1}_model_{i}.pt\"))\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# #     cm = np.asarray(client_models)\n",
    "# fl.server_aggregate(global_model, client_models, client_models)\n",
    "# #     fl.server_aggregate(global_model, client_models)\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"Time taken for server aggregation : {end-start} seconds.\")\n",
    "\n",
    "# # test_loss, test_acc = fl.test(global_model, testloader, device)\n",
    "\n",
    "# print(f\"Round {round_no}\")\n",
    "# # print('Test loss %0.3g - Test accuracy: %0.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# # Plotting loss\n",
    "# f1 = plt.figure()\n",
    "# plt.plot(np.arange(1, num_rounds+1), loss_train_collect, label='Train', linestyle = 'dashed')\n",
    "# plt.plot(np.arange(1, num_rounds+1), loss_test_collect, label='Test', linestyle = 'solid')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# # Plotting accuracy\n",
    "# f2 = plt.figure()\n",
    "# plt.plot(np.arange(1, num_rounds+1), acc_train_collect, label='Train', linestyle = 'dashed')\n",
    "# plt.plot(np.arange(1, num_rounds+1), acc_test_collect, label='Test', linestyle = 'solid')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.grid(True)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# # GPU usage information after computation\n",
    "# if device.type == 'cuda':\n",
    "#     for i in range(0,torch.cuda.device_count()):\n",
    "#         print(torch.cuda.get_device_name(i))\n",
    "#         print('Memory Usage of device :', i)\n",
    "#         print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#         print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING SCRIPT FOR LATENCY OF SRR ON SMALL-VGG\n",
    "\n",
    "\n",
    "# import random\n",
    "\n",
    "\n",
    "# ldp_func = SRR(c = 0.0, r = 0.03, epsilon = 5, delta_d = 30 , precision = 5, m = 10)\n",
    "\n",
    "# start = time.time()\n",
    "# templist = []\n",
    "# for i in range(2000000):\n",
    "#     tempo = ldp_func.perturb(random.uniform(0, 0.075))\n",
    "# #     tempo = LDP_FL(0.0)\n",
    "# #     templist.append(tempo)\n",
    "# #     print(tempo)\n",
    "# # ldp_func.np_perturb(np.zeros(20490))\n",
    "# end = time.time()\n",
    "\n",
    "# # print(f\"Mean Value is {statistics.mean(templist)}.\")\n",
    "# del templist\n",
    "\n",
    "# print(f\"Time taken for 2 million perturbs: {end-start} seconds.\")\n",
    "\n",
    "# model = nn.DataParallel(Net()).cuda()\n",
    "# start = time.time()\n",
    "# for name, param in model.named_parameters():\n",
    "# #                     param.data = torch.tensor(list(map(ldp_funcs[name].perturb, param.data.detach().cpu().flatten()))).reshape(param.shape).to(param.device)\n",
    "#     print(name)\n",
    "#     print(param.shape)\n",
    "#     t1 = time.time()\n",
    "#     param_val = param.data.detach().cpu().flatten().numpy()\n",
    "# #                     param_val = param.data.detach().flatten()\n",
    "#     t2 = time.time()\n",
    "#     print(f\"\\nTime taken for param detach and flatten : {t2-t1} seconds.\")\n",
    "# #                     perturbed_weight = [ldp_funcs[name].perturb(val) for val in param_val]\n",
    "#     for idx in range(len(param_val)):\n",
    "#         param_val[idx] = ldp_func.perturb(param_val[idx])\n",
    "#     t3 = time.time()\n",
    "#     print(f\"Time taken for perturbing param : {t3-t2} seconds.\")\n",
    "# # #                     perturbed_weight = ldp_func.np_perturb(param.data.detach().cpu().flatten())\n",
    "# #                     param.data = torch.tensor(perturbed_weight).reshape(param.shape).to(param.device)\n",
    "#     param.data = torch.tensor(param_val).reshape(param.shape).to(param.device)\n",
    "# #                     param.data = torch.tensor(perturbed_weight).reshape(param.shape)\n",
    "#     t4 = time.time()\n",
    "#     print(f\"Time taken for reshape and assign : {t4-t3} seconds.\\n\")\n",
    "# #                     print(\"One param done!\")\n",
    "# #                     print(name, \"\\n\")\n",
    "# end = time.time()\n",
    "# print(f\"Model {i+1} perturbed!\")\n",
    "# print(f\"Time taken for perturbation : {end-start} seconds.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
